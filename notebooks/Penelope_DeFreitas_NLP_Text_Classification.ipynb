{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Cxi1EHQx9D"
      },
      "source": [
        "# NLP_Text_Classification\n",
        "## This notebook outlines the usage of NLP Feature extraction (CountVectorizer, Word2Vec, Doc2Vec, TfidfVectorizer) in classification of text documents\n",
        "\n",
        "## Algorithms: Multinomial NaÃ¯ve Bayes, Logistic Regression, Support Vector Machines, Decision Trees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vx9mFwNQx9J"
      },
      "source": [
        "### Import all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQZTlxruQx9K"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import logging\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, roc_curve, roc_auc_score, f1_score\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tvOp5VuQx9M"
      },
      "source": [
        "### Choose a few categories fro the entire 20 categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE3IUwGeQx9N"
      },
      "outputs": [],
      "source": [
        "# Load some categories from the training set\n",
        "categories = [\n",
        "    'alt.atheism',\n",
        "    'talk.religion.misc',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbuDvQblQx9N",
        "outputId": "c73e65a7-4131-4769-924f-06ee5b7897b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 20 newsgroups dataset for categories:\n",
            "['alt.atheism', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzdDU5pQx9O"
      },
      "source": [
        "### Fetch documents for these 2 categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWcULn5KQx9P",
        "outputId": "aa8b4f4c-fdf5-44ad-9552-8f6103a4729f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "857 documents\n",
            "2 categories\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "print(f\"{len(data.filenames)} documents\")\n",
        "print(f\"{len(data.target_names)} categories\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK32788zQx9Q",
        "outputId": "45d9c090-a88c-4ebf-faca-4d38dd45bd1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target names: ['alt.atheism', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(\"Target names:\", data.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYHmpmBKQx9Q"
      },
      "outputs": [],
      "source": [
        "print(\"Filenames of the documents:\", data.filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC8b4Cm5Qx9R"
      },
      "source": [
        "## Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rokk5ZMQx9R"
      },
      "outputs": [],
      "source": [
        "# Features (X)\n",
        "X = data.data\n",
        "\n",
        "# Target labels (y)\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0eX9z7aQx9R",
        "outputId": "dc4558e5-77ec-464f-eeae-57b52dffa90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First document:\n",
            "From: mangoe@cs.umd.edu (Charley Wingate)\n",
            "Subject: Benediktine Metaphysics\n",
            "Lines: 24\n",
            "\n",
            "Benedikt Rosenau writes, with great authority:\n",
            "\n",
            ">     IF IT IS CONTRADICTORY IT CANNOT EXIST.\n",
            "\n",
            "\"Contradictory\" is a property of language.  If I correct this to\n",
            "\n",
            "\n",
            "      THINGS DEFINED BY CONTRADICTORY LANGUAGE DO NOT EXIST\n",
            "\n",
            "I will object to definitions as reality.  If you then amend it to\n",
            "\n",
            "      THINGS DESCRIBED BY CONTRADICTORY LANGUAGE DO NOT EXIST\n",
            "\n",
            "then we've come to something which is plainly false.  Failures in\n",
            "description are merely failures in description.\n",
            "\n",
            "(I'm not an objectivist, remember.)\n",
            "\n",
            "\n",
            "-- \n",
            "C. Wingate        + \"The peace of God, it is no peace,\n",
            "                  +    but strife closed in the sod.\n",
            "mangoe@cs.umd.edu +  Yet, brothers, pray for but one thing:\n",
            "tove!mangoe       +    the marv'lous peace of God.\"\n",
            "\n",
            "Target label: 0\n"
          ]
        }
      ],
      "source": [
        "# Print the first document and its target label\n",
        "print(\"First document:\")\n",
        "print(X[0])\n",
        "print(\"Target label:\", y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-PJlERHQx9S",
        "outputId": "a90d7e33-1889-4af5-f23d-9ebed9d96ebd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0NIVfgWQx9S"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXB3Bj47Qx9S"
      },
      "source": [
        "## CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgD3lopbQx9S"
      },
      "outputs": [],
      "source": [
        "# Define the vectorizer and transformers\n",
        "vect = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_q0YfgjQx9S"
      },
      "outputs": [],
      "source": [
        "# Define the classifiers\n",
        "nb_classifier = MultinomialNB()\n",
        "lr_classifier = LogisticRegression(class_weight=\"balanced\",random_state=42)\n",
        "svm_classifier = SVC(random_state=42)\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7KAbatVQx9S"
      },
      "source": [
        "### CountVectorizer with Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-4LLf0AQx9T",
        "outputId": "03f81f2c-f33c-4125-8b0f-15d9f7d323e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9069767441860465\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform the training data\n",
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data (do not fit again, use the vocabulary learned from the training data)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "\n",
        "# Fit the Multinomial Naive Bayes model\n",
        "nb_classifier.fit(X_train_dtm, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_class = nb_classifier.predict(X_test_dtm)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzycorpPQx9T"
      },
      "source": [
        "### CountVectorizer with Linear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fPNz1mQx9T"
      },
      "outputs": [],
      "source": [
        "lr_classifier.fit(X_train_dtm, y_train)\n",
        "y_pred_class = lr_classifier.predict(X_test_dtm)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHlSLXMfQx9T"
      },
      "source": [
        "### CountVectorizer with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na-JVYWzQx9T",
        "outputId": "6bd30dce-70c5-49bf-bd1f-db01c9a5250e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6744186046511628\n"
          ]
        }
      ],
      "source": [
        "svm_classifier.fit(X_train_dtm, y_train)\n",
        "y_pred_class = svm_classifier.predict(X_test_dtm)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPf81s2MQx9T"
      },
      "source": [
        "### CountVectorizer with Decision Tree model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiVVRVQ7Qx9U",
        "outputId": "83843753-8d86-4bde-d9fe-0daedb58d54a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.872093023255814\n"
          ]
        }
      ],
      "source": [
        "dt_classifier.fit(X_train_dtm, y_train)\n",
        "y_pred_class = dt_classifier.predict(X_test_dtm)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS2RzW_AQx9U"
      },
      "source": [
        "## TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DmB8_xpQx9U"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfXzslbmQx9U"
      },
      "source": [
        "### TfidfVectorizer with Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JguFJQ2XQx9U",
        "outputId": "b88a207c-8d02-4d98-ef10-3b58f5e1c10d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7635658914728682\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the text data using TF-IDF\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Create and fit the model\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_class = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy (or other metrics)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFZcb1KeQx9U"
      },
      "source": [
        "### TfidfVectorizer with Linear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp10wYVHQx9U",
        "outputId": "64157507-c7bb-46d3-fed1-cb2b28a8bf60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9186046511627907\n"
          ]
        }
      ],
      "source": [
        "lr_classifier.fit(X_train_tfidf, y_train)\n",
        "y_pred_class = lr_classifier.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIahTNlnQx9V"
      },
      "source": [
        "### TfidfVectorizer with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbPwNLgCQx9V",
        "outputId": "07d4b0d6-9257-4ae5-edf8-dace0eab5360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9341085271317829\n"
          ]
        }
      ],
      "source": [
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "y_pred_class = svm_classifier.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NATH-KGQx9V"
      },
      "source": [
        "### TfidfVectorizer with Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_M_rtpPQx9V",
        "outputId": "f967f531-9340-41c0-862b-d71dca7f6852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8372093023255814\n"
          ]
        }
      ],
      "source": [
        "dt_classifier.fit(X_train_tfidf, y_train)\n",
        "y_pred_class = dt_classifier.predict(X_test_tfidf)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoKHoVbvQx9V"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvMvkWU-Qx9V"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdkiaREQx9V"
      },
      "source": [
        "### Tokenize the docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXIpHxmeQx9a",
        "outputId": "97b4ae66-f865-4347-b945-0684632fc958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['From', ':', 'pmy', '@', 'vivaldi.acc.virginia.edu', '(', 'Pete', 'Yadlowsky', ')', 'Subject', ':', 'Re', ':', 'Who', \"'s\", 'next', '?', 'Mormons', 'and', 'Jews', '?', 'Organization', ':', 'University', 'of', 'Virginia', 'Lines', ':', '17', 'Ken', 'Arromdee', 'writes', '>', '>', 'Did', 'they', 'not', 'know', 'that', 'these', 'men', 'were', 'federal', 'officers', '?', '>', 'Do', 'you', 'know', 'what', 'a', '``', 'no-knock', 'search', \"''\", 'is', '?', 'Yes', ',', 'but', 'tell', 'me', 'how', 'you', 'think', 'your', 'question', 'answers', 'my', 'question', '.', 'If', 'the', 'BDs', 'did', \"n't\", 'know', 'immediately', 'that', 'they', 'were', 'dealing', 'with', 'feds', '(', 'uniform', 'apparel', ',', 'insignia', ')', ',', 'they', 'must', 'have', 'figured', 'it', 'out', 'in', 'pretty', 'short', 'order', '.', 'Why', 'did', 'they', 'keep', 'fighting', '?', 'They', 'seemed', 'awfully', 'ready', 'for', 'having', 'been', 'attacked', '``', 'without', 'warning', \"''\", '.', '--', 'Peter', 'M.', 'Yadlowsky', '|', 'Wake', '!', 'The', 'sky', 'is', 'light', '!', 'Academic', 'Computing', 'Center', '|', 'Let', 'us', 'to', 'the', 'Net', 'again', '...', 'University', 'of', 'Virginia', '|', 'Companion', 'keyboard', '.', 'pmy', '@', 'Virginia.EDU', '|', '-', 'after', 'Basho']\n",
            "['From', ':', 'eczcaw', '@', 'mips.nott.ac.uk', '(', 'A.Wainwright', ')', 'Subject', ':', 'Re', ':', 'some', 'thoughts', '.', 'Keywords', ':', 'Dan', 'Bissell', 'Reply-To', ':', 'eczcaw', '@', 'mips.nott.ac.uk', '(', 'A.Wainwright', ')', 'Organization', ':', 'Nottingham', 'University', 'Lines', ':', '28', 'In', 'article', '<', 'healta.145.734928689', '@', 'saturn.wwc.edu', '>', ',', 'healta', '@', 'saturn.wwc.edu', '(', 'Tammy', 'R', 'Healy', ')', 'writes', ':', '|', '>', 'I', 'hope', 'you', \"'re\", 'not', 'going', 'to', 'flame', 'him', '.', 'Please', 'give', 'him', 'the', 'same', 'coutesy', \"you'\", '|', '>', 've', 'given', 'me', '.', '|', '>', '|', '>', 'Tammy', 'If', 'a', 'person', 'gives', 'a', 'well-balanced', 'reasoned', 'argument', ',', 'Tammy', ',', 'then', 'all', 'are', 'happy', 'to', 'discuss', 'it', 'with', 'him', '.', 'If', 'he', 'makes', 'astounding', 'claims', ',', 'which', 'are', 'not', 'backed', 'up', 'with', 'any', 'evidence', 'then', 'he', 'must', 'be', 'expected', 'to', 'substantiate', 'them', '.', 'If', 'the', 'original', 'author', 'had', 'said', 'that', 'everything', 'was', 'his', 'own', 'opinion', 'and', 'not', 'supportable', 'then', 'people', 'would', 'have', 'simply', 'ignored', 'him', '.', 'He', 'did', 'not', '.', 'He', 'claimed', 'many', 'things', 'and', 'his', 'logic', 'was', 'seriously', 'flawed', '.', 'His', 'argument', 'was', 'for', 'christianity', 'in', 'an', 'effort', 'to', 'try', 'to', 'convince', 'atheists', 'like', 'myself', 'to', 'believe', 'him', 'and', 'his', 'message', '.', 'I', 'for', 'one', 'will', 'not', 'take', 'things', 'as', 'read', '.', 'If', 'you', 'told', 'me', 'that', 'pink', 'fluffy', 'elephants', 'did', 'the', 'dance', 'of', 'the', 'sugar', 'plum', 'fairy', 'on', 'the', 'dark', 'side', 'of', 'Jupiter', 'then', 'I', 'would', 'demand', 'evidence', '!', 'Adda', '--', '+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+', '|', 'Adda', 'Wainwright', '|', 'Does', 'dim', 'atal', 'y', 'llanw', '!', '8o', ')', '|', '|', 'eczcaw', '@', 'mips.nott.ac.uk', '|', '8o', ')', 'Mae', '.sig', \"'ma\", 'ar', 'werth', '!', '|', '+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X_train is a list of documents\n",
        "tokenized_train = [word_tokenize(doc) for doc in X_train]\n",
        "\n",
        "# Print tokens for the first document as an example\n",
        "print(tokenized_train[0])\n",
        "\n",
        "\n",
        "# X_test is a list of documents\n",
        "tokenized_test = [word_tokenize(doc) for doc in X_test]\n",
        "\n",
        "# Print tokens for the first document as an example\n",
        "print(tokenized_test[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfLCPw-xQx9b"
      },
      "source": [
        "### Word2Vec with Gaussian Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3p9nDylQx9b",
        "outputId": "f09fa7b2-0663-4f14-c6d6-d5378bde8b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.46124031007751937\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v.build_vocab(tokenized_train)\n",
        "w2v.train(tokenized_train, total_examples=w2v.corpus_count, epochs=10)\n",
        "\n",
        "# Transform the data using the trained Word2Vec model\n",
        "X_train_w2v = [np.mean([w2v.wv[word] for word in doc if word in w2v.wv], axis=0) for doc in tokenized_train]\n",
        "X_test_w2v = [np.mean([w2v.wv[word] for word in doc if word in w2v.wv], axis=0) for doc in tokenized_test]\n",
        "\n",
        "# Create and fit the model\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_class = gnb_classifier.predict(X_test_w2v)\n",
        "\n",
        "# Calculate accuracy (or other metrics)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ji5e2VRQx9c"
      },
      "source": [
        "### Word2Vec with Linear Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHxJLRnBQx9c",
        "outputId": "30e1c630-7c41-48e8-cabf-1ae1c72f03b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7093023255813954\n"
          ]
        }
      ],
      "source": [
        "lr_classifier.fit(X_train_w2v, y_train)\n",
        "y_pred_class = lr_classifier.predict(X_test_w2v)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0J1edgfQx9c"
      },
      "source": [
        "### Word2Vec with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAZDUpO9Qx9c",
        "outputId": "f9a5c2e2-35af-413b-da40-3371937f55ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5930232558139535\n"
          ]
        }
      ],
      "source": [
        "svm_classifier.fit(X_train_w2v, y_train)\n",
        "y_pred_class = svm_classifier.predict(X_test_w2v)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daeAXEo5Qx9c"
      },
      "source": [
        "### Word2Vec with Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7IMMyXPQx9c",
        "outputId": "c7ae9eea-62f3-4cec-ee45-35f033af66ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6046511627906976\n"
          ]
        }
      ],
      "source": [
        "dt_classifier.fit(X_train_vectors, y_train)\n",
        "y_pred_class = dt_classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2nOkrWpQx9c"
      },
      "source": [
        "## Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_xrT4eoQx9d"
      },
      "outputs": [],
      "source": [
        "d2v = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfiLKlCYQx9d"
      },
      "source": [
        "### Doc2Vec with Gaussian Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpBPHk2EQx9d",
        "outputId": "9fd9bf7d-5dbe-4379-e551-74d8615959f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6395348837209303\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Tag documents\n",
        "X_train_tagged = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(X_train)]\n",
        "X_test_tagged = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(X_test)]\n",
        "\n",
        "# Step 2: Train Doc2Vec model\n",
        "d2v.build_vocab(X_train_tagged)\n",
        "d2v.train(X_train_tagged, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
        "\n",
        "# Step 3: Infer vectors\n",
        "X_train_vectors = [d2v.infer_vector(doc.words) for doc in X_train_tagged]\n",
        "X_test_vectors = [d2v.infer_vector(doc.words) for doc in X_test_tagged]\n",
        "\n",
        "# Step 4: Train Gaussian Naive Bayes\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Step 5: Predict and Evaluate\n",
        "y_pred_class = gnb_classifier.predict(X_test_vectors)\n",
        "\n",
        "# Calculate accuracy (or other metrics)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymyRLOn7Qx9d"
      },
      "source": [
        "### Doc2Vec with Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj6MGspAQx9d",
        "outputId": "c655a003-b04e-430a-9643-7386c9c2832d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6937984496124031\n"
          ]
        }
      ],
      "source": [
        "lr_classifier.fit(X_train_vectors, y_train)\n",
        "y_pred_class = lr_classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn8A3dK2Qx9d"
      },
      "source": [
        "### Doc2Vec with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwaljETFQx9e",
        "outputId": "1aed2d48-7eb7-4640-9748-015b79a48e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6511627906976745\n"
          ]
        }
      ],
      "source": [
        "svm_classifier.fit(X_train_vectors, y_train)\n",
        "y_pred_class = svm_classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU0jU7RnQx9e"
      },
      "source": [
        "### Doc2Vec with Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPwqfk7fQx9e",
        "outputId": "9b5abb3e-e43b-44af-9e5e-ec7de05c47eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6046511627906976\n"
          ]
        }
      ],
      "source": [
        "dt_classifier.fit(X_train_vectors, y_train)\n",
        "y_pred_class = dt_classifier.predict(X_test_vectors)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epRI_x2OQx9e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}